{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThis package provides an implementation of an observable for Markov Chain Monte Carlo simulations (like the currently out-dated \nMonteCarlo.jl\n).\n\n\nDuring a \nMarkov chain Monte Carlo simulation\n a Markov walker (after thermalization) walks through configuration space according to the equilibrium distribution. Typically, one measures observables along the Markov path, records the results, and in the end averages the measurements. \nMonteCarloObservable.jl\n provides all the necessary tools for conveniently conducting these types of measurements, including estimation of one-sigma error bars through binning or jackknife analysis.\n\n\n\n\nInstallation\n\n\nIn the REPL, switch to pkg mode (by pressing \n]\n) and enter\n\n\nadd\n \nhttps\n://\ngithub\n.\ncom\n/\ncrstnbr\n/\nMonteCarloObservable\n.\njl\n\n\n\n\n\n\nAlternatively, you can install the package per\n\n\nusing\n \nPkg\n\n\nPkg\n.\nadd\n(\nhttps://github.com/crstnbr/MonteCarloObservable.jl\n)", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "This package provides an implementation of an observable for Markov Chain Monte Carlo simulations (like the currently out-dated  MonteCarlo.jl ).  During a  Markov chain Monte Carlo simulation  a Markov walker (after thermalization) walks through configuration space according to the equilibrium distribution. Typically, one measures observables along the Markov path, records the results, and in the end averages the measurements.  MonteCarloObservable.jl  provides all the necessary tools for conveniently conducting these types of measurements, including estimation of one-sigma error bars through binning or jackknife analysis.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#installation", 
            "text": "In the REPL, switch to pkg mode (by pressing  ] ) and enter  add   https :// github . com / crstnbr / MonteCarloObservable . jl   Alternatively, you can install the package per  using   Pkg  Pkg . add ( https://github.com/crstnbr/MonteCarloObservable.jl )", 
            "title": "Installation"
        }, 
        {
            "location": "/manual/gettingstarted/", 
            "text": "Getting started\n\n\nSometimes an example says more than a thousand words. So let's start with one.\n\n\n\n\nSimple Example\n\n\nThis is a basic demontration of how to use the package:\n\n\njulia\n using MonteCarloObservable\n\njulia\n obs = Observable(Float64, \nmyobservable\n)\nObservable \nmyobservable\n of type Float64 with 0 measurements\n\njulia\n add!(obs, 1.23) # add measurement\n\njulia\n obs\nObservable \nmyobservable\n of type  Float64 with 1 measurement\n\njulia\n push!(obs, rand(4)) # same as add!\n\njulia\n length(obs)\n5\n\njulia\n timeseries(obs)\n5-element Array{Float64,1}:\n 1.23\n 0.32230475959913885\n 0.492257520850492\n 0.4279399452983663\n 0.3051279572653134\n\njulia\n obs[3] # conventional element accessing\n0.492257520850492\n\njulia\n obs[end-2:end]\n3-element Array{Float64,1}:\n 0.492257520850492\n 0.4279399452983663\n 0.3051279572653134\n\njulia\n add!(obs, rand(995))\n\njulia\n mean(obs)\n0.49559897908784795\n\njulia\n error(obs) # one-sigma error of mean (binning analysis)\n0.009027458601456049\n\njulia\n saveobs(obs, \nmyobservable.jld\n)\n\n\n\n\n\n\n\nCreating \nObservable\ns\n\n\nTODO:\n mention all important keywords \nTODO:\n mention \nalloc\n keyword and importance of preallocation. \nTODO:\n mention \n@obs\n and \n@diskobs\n macros", 
            "title": "Getting started"
        }, 
        {
            "location": "/manual/gettingstarted/#getting-started", 
            "text": "Sometimes an example says more than a thousand words. So let's start with one.", 
            "title": "Getting started"
        }, 
        {
            "location": "/manual/gettingstarted/#simple-example", 
            "text": "This is a basic demontration of how to use the package:  julia  using MonteCarloObservable\n\njulia  obs = Observable(Float64,  myobservable )\nObservable  myobservable  of type Float64 with 0 measurements\n\njulia  add!(obs, 1.23) # add measurement\n\njulia  obs\nObservable  myobservable  of type  Float64 with 1 measurement\n\njulia  push!(obs, rand(4)) # same as add!\n\njulia  length(obs)\n5\n\njulia  timeseries(obs)\n5-element Array{Float64,1}:\n 1.23\n 0.32230475959913885\n 0.492257520850492\n 0.4279399452983663\n 0.3051279572653134\n\njulia  obs[3] # conventional element accessing\n0.492257520850492\n\njulia  obs[end-2:end]\n3-element Array{Float64,1}:\n 0.492257520850492\n 0.4279399452983663\n 0.3051279572653134\n\njulia  add!(obs, rand(995))\n\njulia  mean(obs)\n0.49559897908784795\n\njulia  error(obs) # one-sigma error of mean (binning analysis)\n0.009027458601456049\n\njulia  saveobs(obs,  myobservable.jld )", 
            "title": "Simple Example"
        }, 
        {
            "location": "/manual/gettingstarted/#creating-observables", 
            "text": "TODO:  mention all important keywords  TODO:  mention  alloc  keyword and importance of preallocation.  TODO:  mention  @obs  and  @diskobs  macros", 
            "title": "Creating Observables"
        }, 
        {
            "location": "/manual/errorestimation/", 
            "text": "Error estimation\n\n\nAutomatic estimation of the \nstandard error of the mean\n (one-sigma error bars) is based on a binning analysis.\n\n\n\n\nStandard error versus standard deviation\n\n\nBe careful not to confuse the terms \"standard error (of the mean)\" and \"standard deviation\". Quoting \nWikipedia\n on this:\n\n\n\n\nPut simply, the \nstandard error\n of the sample mean is an estimate of how far the sample mean is likely to be from the population mean, whereas the \nstandard deviation\n of the sample is the degree to which individuals within the sample differ from the sample mean. If the population \nstandard deviation\n is finite, the \nstandard error\n of the mean of the sample will tend to zero with increasing sample size, because the estimate of the population mean will improve, while the \nstandard deviation\n of the sample will tend to approximate the population \nstandard deviation\n as the sample size increases.\n\n\n\n\nThe standard error of an observable can be obtained by \nerror(obs)\n and the standard deviation by \nstd(obs)\n.\n\n\n\n\n\n\nBinning analysis\n\n\nFor \nN\nN\n uncorrelated measurements of an observable \nO\nO\n the statistical standard error \n\\sigma\n\\sigma\n, the root-mean-square deviation of the time series mean from the true mean, falls off with the number of measurements \nN\nN\n according to\n\n\n\n\n\n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},\n\n\n\n\n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},\n\n\n\n\n\nwhere \n\\sigma_{O}\n\\sigma_{O}\n is the standard deviation of the observable \nO\nO\n.\n\n\nIn a Markov chain Monte Carlo sampling, however, measurements are usually correlated due to the fact that the next step of the Markov walker depends on his current position in configuration space. One way to estimate the statistical error in this case is by binning analysis. The idea is to partition the time series into bins of a fixed size large enough such that neighboring bins are uncorrelated, that is there means are uncorrelated. For this procedure to be reliable we need both a large bin size (larger than the Markov time scale of correlations, typically called autocorrelation time) and many bins (to suppress statistical fluctuations).\n\n\nThe typical procedure is to look at the estimate for the statistical error as a function of bin size and expect a plateau (convergence of the estimate). You can do this manually using \nerrorplot(obs)\n. Automatically, the package uses a plateau-finder algorithm to check wether convergence has been reached. Note, however, that finding a plateau (with expected fluctuations) numerically in an automated manner isn't trivial. Hence, the algorithm is somewhat heuristic as it is in other software like \nALPS\n and shouldn't be trusted without further manual checking. It is conservative though in the sense that it tends to be rather false-negative than false-positive.\n\n\nFrom this we conclude that estimates for the error really only become reliable in the limit of many measurements.\n\n\n\n\nReferences\n\n\nJ. Gubernatis, N. Kawashima, and P. Werner, \nQuantum Monte Carlo Methods: Algorithms for Lattice Models\n, Book (2016)\n\n\nV. Ambegaokar, and M. Troyer, \nEstimating errors reliably in Monte Carlo simulations of the Ehrenfest model\n, American Journal of Physics \n78\n, 150 (2010)\n\n\n\n\nJackknife analysis\n\n\nSee for example the corresponding \nWikipedia article\n.", 
            "title": "Error estimation"
        }, 
        {
            "location": "/manual/errorestimation/#error-estimation", 
            "text": "Automatic estimation of the  standard error of the mean  (one-sigma error bars) is based on a binning analysis.   Standard error versus standard deviation  Be careful not to confuse the terms \"standard error (of the mean)\" and \"standard deviation\". Quoting  Wikipedia  on this:   Put simply, the  standard error  of the sample mean is an estimate of how far the sample mean is likely to be from the population mean, whereas the  standard deviation  of the sample is the degree to which individuals within the sample differ from the sample mean. If the population  standard deviation  is finite, the  standard error  of the mean of the sample will tend to zero with increasing sample size, because the estimate of the population mean will improve, while the  standard deviation  of the sample will tend to approximate the population  standard deviation  as the sample size increases.   The standard error of an observable can be obtained by  error(obs)  and the standard deviation by  std(obs) .", 
            "title": "Error estimation"
        }, 
        {
            "location": "/manual/errorestimation/#binning-analysis", 
            "text": "For  N N  uncorrelated measurements of an observable  O O  the statistical standard error  \\sigma \\sigma , the root-mean-square deviation of the time series mean from the true mean, falls off with the number of measurements  N N  according to   \n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},  \n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},   where  \\sigma_{O} \\sigma_{O}  is the standard deviation of the observable  O O .  In a Markov chain Monte Carlo sampling, however, measurements are usually correlated due to the fact that the next step of the Markov walker depends on his current position in configuration space. One way to estimate the statistical error in this case is by binning analysis. The idea is to partition the time series into bins of a fixed size large enough such that neighboring bins are uncorrelated, that is there means are uncorrelated. For this procedure to be reliable we need both a large bin size (larger than the Markov time scale of correlations, typically called autocorrelation time) and many bins (to suppress statistical fluctuations).  The typical procedure is to look at the estimate for the statistical error as a function of bin size and expect a plateau (convergence of the estimate). You can do this manually using  errorplot(obs) . Automatically, the package uses a plateau-finder algorithm to check wether convergence has been reached. Note, however, that finding a plateau (with expected fluctuations) numerically in an automated manner isn't trivial. Hence, the algorithm is somewhat heuristic as it is in other software like  ALPS  and shouldn't be trusted without further manual checking. It is conservative though in the sense that it tends to be rather false-negative than false-positive.  From this we conclude that estimates for the error really only become reliable in the limit of many measurements.", 
            "title": "Binning analysis"
        }, 
        {
            "location": "/manual/errorestimation/#references", 
            "text": "J. Gubernatis, N. Kawashima, and P. Werner,  Quantum Monte Carlo Methods: Algorithms for Lattice Models , Book (2016)  V. Ambegaokar, and M. Troyer,  Estimating errors reliably in Monte Carlo simulations of the Ehrenfest model , American Journal of Physics  78 , 150 (2010)", 
            "title": "References"
        }, 
        {
            "location": "/manual/errorestimation/#jackknife-analysis", 
            "text": "See for example the corresponding  Wikipedia article .", 
            "title": "Jackknife analysis"
        }, 
        {
            "location": "/manual/memdisk/", 
            "text": "Memory / disk storage\n\n\nBy default the full time series of an observable is kept in memory. This is the most convenient option as it renders element access and error computation fast. However, sometimes it is preferable to track the time series on disk rather than completely in memory:\n\n\n\n\nAbrupt termination: the simulation might be computationally expensive, thus slow, and might abort abruptly (maybe due to cluster outage or time limit). In this case, one probably wants to have a restorable \"memory dump\" of the so far recorded measurements to not have to restart from scratch.\n\n\nMemory limit: the tracked observable might be large, i.e. a large complex matrix. Then, storing a long time series might make the simulation exceed a memory limit (and often stop unexpectedly). Keeping the time series memory on disk solves this problem.\n\n\n\n\n\n\n\"Disk observables\"\n\n\nA \"disk observable\" is an \nObservable\n that every once in a while dumps it's time series memory to disk and only keeps the latest data points in memory. You can create a \"disk observable\" as\n\n\nobs\n \n=\n \nObservable\n(\nFloat64\n,\n \nmyobservable\n;\n \ninmemory\n=\nfalse\n,\n \nalloc\n=\n100\n)\n\n\n\n\n\n\nIt will record measurements in memory until the preallocated time series buffer (\nalloc=100\n) overflows in which case it saves it's time series memory to disk (\noutfile\n). In the above example this will happen after 100 measurements.\n\n\nApart from the special construction (\ninmemory=false\n) everything else stays the same as for default in-memory observables. For example, we can still get the mean via \nmean(obs)\n, access time series elements with \nobs[idx]\n and load the full time series to memory at any point through \ntimeseries(obs)\n.\n\n\n\n\nNote\n\n\nThe observable's memory dump contains meta information, like name, element type, element size etc., as well as time series memory chunks. The dumping is implemented in the not exported method \nMonteCarloObservable.updateondisk\n. Note that the observable's memory is \nnot\n a full backup of the observable itself (see \nsaveobs\n). Should the simulation terminate abruptly one can nonetheless restore most of the so-far recorded information using \nloadobs_frommemory\n and \ntimeseries_frommemory\n. Measurements that hadn't been dumped yet, because they were still in the preallocated buffer, are lost though.", 
            "title": "Memory / disk storage"
        }, 
        {
            "location": "/manual/memdisk/#memory-disk-storage", 
            "text": "By default the full time series of an observable is kept in memory. This is the most convenient option as it renders element access and error computation fast. However, sometimes it is preferable to track the time series on disk rather than completely in memory:   Abrupt termination: the simulation might be computationally expensive, thus slow, and might abort abruptly (maybe due to cluster outage or time limit). In this case, one probably wants to have a restorable \"memory dump\" of the so far recorded measurements to not have to restart from scratch.  Memory limit: the tracked observable might be large, i.e. a large complex matrix. Then, storing a long time series might make the simulation exceed a memory limit (and often stop unexpectedly). Keeping the time series memory on disk solves this problem.", 
            "title": "Memory / disk storage"
        }, 
        {
            "location": "/manual/memdisk/#disk-observables", 
            "text": "A \"disk observable\" is an  Observable  that every once in a while dumps it's time series memory to disk and only keeps the latest data points in memory. You can create a \"disk observable\" as  obs   =   Observable ( Float64 ,   myobservable ;   inmemory = false ,   alloc = 100 )   It will record measurements in memory until the preallocated time series buffer ( alloc=100 ) overflows in which case it saves it's time series memory to disk ( outfile ). In the above example this will happen after 100 measurements.  Apart from the special construction ( inmemory=false ) everything else stays the same as for default in-memory observables. For example, we can still get the mean via  mean(obs) , access time series elements with  obs[idx]  and load the full time series to memory at any point through  timeseries(obs) .   Note  The observable's memory dump contains meta information, like name, element type, element size etc., as well as time series memory chunks. The dumping is implemented in the not exported method  MonteCarloObservable.updateondisk . Note that the observable's memory is  not  a full backup of the observable itself (see  saveobs ). Should the simulation terminate abruptly one can nonetheless restore most of the so-far recorded information using  loadobs_frommemory  and  timeseries_frommemory . Measurements that hadn't been dumped yet, because they were still in the preallocated buffer, are lost though.", 
            "title": "\"Disk observables\""
        }, 
        {
            "location": "/methods/general/", 
            "text": "Methods: General\n\n\nBelow you find all general exports.\n\n\n\n\nIndex\n\n\n\n\nMonteCarloObservable.Observable\n\n\nBase.eltype\n\n\nBase.getindex\n\n\nBase.isempty\n\n\nBase.length\n\n\nBase.ndims\n\n\nBase.push!\n\n\nBase.push!\n\n\nBase.size\n\n\nBase.view\n\n\nDistributed.clear!\n\n\nMonteCarloObservable.add!\n\n\nMonteCarloObservable.add!\n\n\nMonteCarloObservable.inmemory\n\n\nMonteCarloObservable.name\n\n\nMonteCarloObservable.rename\n\n\nMonteCarloObservable.reset!\n\n\nMonteCarloObservable.timeseries\n\n\nMonteCarloObservable.@diskobs\n\n\nMonteCarloObservable.@obs\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nMonteCarloObservable.Observable\n \n \nMethod\n.\n\n\nObservable(t, name; keyargs...)\n\n\n\n\n\nCreate an observable of type \nt\n.\n\n\nThe following keywords are allowed:\n\n\n\n\nalloc\n: preallocated size of time series container\n\n\noutfile\n: default HDF5/JLD output file for io operations\n\n\ndataset\n: target path within \noutfile\n\n\ninmemory\n: wether to keep the time series in memory or on disk\n\n\nmeantype\n: type of the mean (should be compatible with measurement type \nt\n)\n\n\n\n\nSee also \nObservable\n.\n\n\n#\n\n\nMonteCarloObservable.@diskobs\n \n \nMacro\n.\n\n\nConvenience macro for generating a \"disk observable\" (\ninmemory=false\n) from a vector of measurements.\n\n\n#\n\n\nMonteCarloObservable.@obs\n \n \nMacro\n.\n\n\nConvenience macro for generating an Observable from a vector of measurements.\n\n\n#\n\n\nBase.eltype\n \n \nMethod\n.\n\n\neltype(obs::Observable{T})\n\n\n\n\n\nReturns the type \nT\n of a measurment of the observable.\n\n\n#\n\n\nBase.getindex\n \n \nMethod\n.\n\n\ngetindex(obs::Observable{T}, args...)\n\n\n\n\n\nGet an element of the measurement time series of the observable.\n\n\n#\n\n\nBase.isempty\n \n \nMethod\n.\n\n\nisempty(obs::Observable{T})\n\n\n\n\n\nDetermine wether the observable hasn't been measured yet.\n\n\n#\n\n\nBase.length\n \n \nMethod\n.\n\n\nlength(obs::Observable{T})\n\n\n\n\n\nNumber of measurements of the observable.\n\n\n#\n\n\nBase.ndims\n \n \nMethod\n.\n\n\nndims(obs::Observable{T})\n\n\n\n\n\nNumber of dimensions of the observable (of one measurement).\n\n\nEquivalent to \nndims(T)\n.\n\n\n#\n\n\nBase.push!\n \n \nMethod\n.\n\n\npush\n!(\nobs\n::\nObservable\n{\nT\n}\n,\n \nmeasurements\n::\nAbstractArray\n{\nT\n}\n;\n \nverbose\n=\nfalse\n)\n\n\n\n\n\n\nAdd multiple \nmeasurements\n to observable \nobs\n. Note that because of preallocation this isn't really a push.\n\n\n#\n\n\nBase.push!\n \n \nMethod\n.\n\n\npush\n!(\nobs\n::\nObservable\n{\nT\n}\n,\n \nmeasurement\n::\nT\n;\n \nverbose\n=\nfalse\n)\n\n\n\n\n\n\nAdd a \nmeasurement\n to observable \nobs\n. Note that because of preallocation this isn't really a push.\n\n\n#\n\n\nBase.size\n \n \nMethod\n.\n\n\nsize(obs::Observable{T})\n\n\n\n\n\nSize of the observable (of one measurement).\n\n\n#\n\n\nBase.view\n \n \nMethod\n.\n\n\nview(obs::Observable{T}, args...)\n\n\n\n\n\nGet a view into the measurement time series of the observable.\n\n\n#\n\n\nDistributed.clear!\n \n \nMethod\n.\n\n\nclear!(obs::Observable{T})\n\n\n\n\n\nClears all measurement information in \nobs\n. Identical to \nreset!\n.\n\n\n#\n\n\nMonteCarloObservable.add!\n \n \nMethod\n.\n\n\nadd\n!(\nobs\n::\nObservable\n{\nT\n}\n,\n \nmeasurements\n::\nAbstractArray\n{\nT\n}\n;\n \nverbose\n=\nfalse\n)\n\n\n\n\n\n\nAdd multiple \nmeasurements\n to observable \nobs\n.\n\n\n#\n\n\nMonteCarloObservable.add!\n \n \nMethod\n.\n\n\nadd\n!(\nobs\n::\nObservable\n{\nT\n}\n,\n \nmeasurement\n::\nT\n;\n \nverbose\n=\nfalse\n)\n\n\n\n\n\n\nAdd a \nmeasurement\n to observable \nobs\n.\n\n\n#\n\n\nMonteCarloObservable.inmemory\n \n \nMethod\n.\n\n\ninmemory(obs::Observable{T})\n\n\n\n\n\nChecks wether the observable is kept in memory (vs. on disk).\n\n\n#\n\n\nMonteCarloObservable.name\n \n \nMethod\n.\n\n\nname(obs::Observable{T})\n\n\n\n\n\nReturns the name of the observable.\n\n\n#\n\n\nMonteCarloObservable.rename\n \n \nMethod\n.\n\n\nrename(obs::Observable{T}, name)\n\n\n\n\n\nRenames the observable.\n\n\n#\n\n\nMonteCarloObservable.reset!\n \n \nMethod\n.\n\n\nreset!(obs::Observable{T})\n\n\n\n\n\nResets all measurement information in \nobs\n. Identical to \nclear!\n.\n\n\n#\n\n\nMonteCarloObservable.timeseries\n \n \nMethod\n.\n\n\ntimeseries(obs::Observable{T})\n\n\n\n\n\nReturns the measurement time series of an observable.\n\n\nIf \ninmemory(obs) == false\n it will read the time series from disk and thus might take some time.\n\n\nSee also \ngetindex\n and \nview\n.", 
            "title": "General"
        }, 
        {
            "location": "/methods/general/#methods-general", 
            "text": "Below you find all general exports.", 
            "title": "Methods: General"
        }, 
        {
            "location": "/methods/general/#index", 
            "text": "MonteCarloObservable.Observable  Base.eltype  Base.getindex  Base.isempty  Base.length  Base.ndims  Base.push!  Base.push!  Base.size  Base.view  Distributed.clear!  MonteCarloObservable.add!  MonteCarloObservable.add!  MonteCarloObservable.inmemory  MonteCarloObservable.name  MonteCarloObservable.rename  MonteCarloObservable.reset!  MonteCarloObservable.timeseries  MonteCarloObservable.@diskobs  MonteCarloObservable.@obs", 
            "title": "Index"
        }, 
        {
            "location": "/methods/general/#documentation", 
            "text": "#  MonteCarloObservable.Observable     Method .  Observable(t, name; keyargs...)  Create an observable of type  t .  The following keywords are allowed:   alloc : preallocated size of time series container  outfile : default HDF5/JLD output file for io operations  dataset : target path within  outfile  inmemory : wether to keep the time series in memory or on disk  meantype : type of the mean (should be compatible with measurement type  t )   See also  Observable .  #  MonteCarloObservable.@diskobs     Macro .  Convenience macro for generating a \"disk observable\" ( inmemory=false ) from a vector of measurements.  #  MonteCarloObservable.@obs     Macro .  Convenience macro for generating an Observable from a vector of measurements.  #  Base.eltype     Method .  eltype(obs::Observable{T})  Returns the type  T  of a measurment of the observable.  #  Base.getindex     Method .  getindex(obs::Observable{T}, args...)  Get an element of the measurement time series of the observable.  #  Base.isempty     Method .  isempty(obs::Observable{T})  Determine wether the observable hasn't been measured yet.  #  Base.length     Method .  length(obs::Observable{T})  Number of measurements of the observable.  #  Base.ndims     Method .  ndims(obs::Observable{T})  Number of dimensions of the observable (of one measurement).  Equivalent to  ndims(T) .  #  Base.push!     Method .  push !( obs :: Observable { T } ,   measurements :: AbstractArray { T } ;   verbose = false )   Add multiple  measurements  to observable  obs . Note that because of preallocation this isn't really a push.  #  Base.push!     Method .  push !( obs :: Observable { T } ,   measurement :: T ;   verbose = false )   Add a  measurement  to observable  obs . Note that because of preallocation this isn't really a push.  #  Base.size     Method .  size(obs::Observable{T})  Size of the observable (of one measurement).  #  Base.view     Method .  view(obs::Observable{T}, args...)  Get a view into the measurement time series of the observable.  #  Distributed.clear!     Method .  clear!(obs::Observable{T})  Clears all measurement information in  obs . Identical to  reset! .  #  MonteCarloObservable.add!     Method .  add !( obs :: Observable { T } ,   measurements :: AbstractArray { T } ;   verbose = false )   Add multiple  measurements  to observable  obs .  #  MonteCarloObservable.add!     Method .  add !( obs :: Observable { T } ,   measurement :: T ;   verbose = false )   Add a  measurement  to observable  obs .  #  MonteCarloObservable.inmemory     Method .  inmemory(obs::Observable{T})  Checks wether the observable is kept in memory (vs. on disk).  #  MonteCarloObservable.name     Method .  name(obs::Observable{T})  Returns the name of the observable.  #  MonteCarloObservable.rename     Method .  rename(obs::Observable{T}, name)  Renames the observable.  #  MonteCarloObservable.reset!     Method .  reset!(obs::Observable{T})  Resets all measurement information in  obs . Identical to  clear! .  #  MonteCarloObservable.timeseries     Method .  timeseries(obs::Observable{T})  Returns the measurement time series of an observable.  If  inmemory(obs) == false  it will read the time series from disk and thus might take some time.  See also  getindex  and  view .", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/statistics/", 
            "text": "Methods: Statistics\n\n\nBelow you find all statistics related exports.\n\n\n\n\nIndex\n\n\n\n\nBase.error\n\n\nMonteCarloObservable.binning_error\n\n\nMonteCarloObservable.binning_error\n\n\nMonteCarloObservable.error_naive\n\n\nMonteCarloObservable.error_with_convergence\n\n\nMonteCarloObservable.iswithinerrorbars\n\n\nMonteCarloObservable.iswithinerrorbars\n\n\nMonteCarloObservable.jackknife_error\n\n\nMonteCarloObservable.tau\n\n\nStatistics.mean\n\n\nStatistics.std\n\n\nStatistics.var\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nBase.error\n \n \nMethod\n.\n\n\nerror(obs::Observable{T})\n\n\n\n\n\nEstimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.\n\n\nNote that this is not the same as \nBase.std(timeseries(obs))\n, not even for uncorrelated measurements.\n\n\nSee also \nmean(obs)\n.\n\n\n#\n\n\nMonteCarloObservable.error_naive\n \n \nMethod\n.\n\n\nerror_naive(obs::Observable{T})\n\n\n\n\n\nEstimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.\n\n\nStrategy: just take largest R value considering an upper limit for bin size (min_nbins)\n\n\n#\n\n\nMonteCarloObservable.error_with_convergence\n \n \nMethod\n.\n\n\nReturns one sigma error and convergence flag (boolean).\n\n\n#\n\n\nMonteCarloObservable.iswithinerrorbars\n \n \nMethod\n.\n\n\niswithinerrorbars(A::AbstractArray{T\n:Number}, B::AbstractArray{T\n:Number}, \u0394::AbstractArray{\n:Real}[, print=false])\n\n\n\n\n\nElementwise check whether \nA\n and \nB\n are equal up to given real error matrix \n\u0394\n. Will print \nA \u2248 B + K.*\u0394\n for \nprint=true\n.\n\n\n#\n\n\nMonteCarloObservable.iswithinerrorbars\n \n \nMethod\n.\n\n\niswithinerrorbars(a, b, \u03b4[, print=false])\n\n\n\n\n\nChecks whether numbers \na\n and \nb\n are equal up to given error \n\u03b4\n. Will print \nx \u2248 y + k\u00b7\u03b4\n for \nprint=true\n.\n\n\nIs equivalent to \nisapprox(a,b,atol=\u03b4,rtol=zero(b))\n.\n\n\n#\n\n\nMonteCarloObservable.jackknife_error\n \n \nMethod\n.\n\n\njackknife_error(g::Function, obs1, ob2, ...)\n\n\n\n\n\nComputes the jackknife one sigma error of \ng(\nobs1\n, \nobs2\n, ...)\n by performing  a \"leave-one-out\" analysis.\n\n\nThe function \ng(x)\n must take one matrix argument \nx\n, whose columns correspond  to the time series of the observables, and produce a scalar (point estimate).\n\n\nExample:\n\n\ng(x) = @views mean(x[:,1])^2 - mean(x[:,2].^2)\n followed by \njackknife_error(g, obs1, obs2)\n. Here \nx[:,1]\n is basically \ntimeseries(obs1)\n and \nx[:,2]\n is \ntimeseries(obs2)\n.\n\n\n#\n\n\nMonteCarloObservable.tau\n \n \nMethod\n.\n\n\ntau(obs::Observable{T})\n\n\n\n\n\nIntegrated autocorrelation time (obtained by binning analysis).\n\n\nSee also \nerror(obs)\n.\n\n\n#\n\n\nStatistics.mean\n \n \nMethod\n.\n\n\nmean(obs::Observable{T})\n\n\n\n\n\nEstimate of the mean of the observable.\n\n\n#\n\n\nStatistics.std\n \n \nMethod\n.\n\n\nstd(obs::Observable{T})\n\n\n\n\n\nStandard deviation of the time series (assuming uncorrelated data).\n\n\nSee also \nmean(obs)\n, \nvar(obs)\n, and \nerror(obs)\n.\n\n\n#\n\n\nStatistics.var\n \n \nMethod\n.\n\n\nvar(obs::Observable{T})\n\n\n\n\n\nVariance of the time series (assuming uncorrelated data).\n\n\nSee also \nmean(obs)\n, \nstd(obs)\n, and \nerror(obs)\n.\n\n\n#\n\n\nMonteCarloObservable.binning_error\n \n \nMethod\n.\n\n\nbinning_error(X)\n\n\n\n\n\nEstimate of the one-sigma error of the time series's mean. Respects correlations between measurements through binning analysis.\n\n\nNote that this is not the same as \nBase.std(X)\n, not even for uncorrelated measurements.\n\n\nFor more details, see \nMonteCarloObservable.Rplateaufinder\n.\n\n\n#\n\n\nMonteCarloObservable.binning_error\n \n \nMethod\n.\n\n\nbinning_error(X, binsize)\n\n\n\n\n\nEstimate of the one-sigma error of the time series's mean. Respect correlations between measurements through binning analysis,  using the given \nbinsize\n (i.e. assuming independence of bins, Eq. 3.18 basically).", 
            "title": "Statistics"
        }, 
        {
            "location": "/methods/statistics/#methods-statistics", 
            "text": "Below you find all statistics related exports.", 
            "title": "Methods: Statistics"
        }, 
        {
            "location": "/methods/statistics/#index", 
            "text": "Base.error  MonteCarloObservable.binning_error  MonteCarloObservable.binning_error  MonteCarloObservable.error_naive  MonteCarloObservable.error_with_convergence  MonteCarloObservable.iswithinerrorbars  MonteCarloObservable.iswithinerrorbars  MonteCarloObservable.jackknife_error  MonteCarloObservable.tau  Statistics.mean  Statistics.std  Statistics.var", 
            "title": "Index"
        }, 
        {
            "location": "/methods/statistics/#documentation", 
            "text": "#  Base.error     Method .  error(obs::Observable{T})  Estimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.  Note that this is not the same as  Base.std(timeseries(obs)) , not even for uncorrelated measurements.  See also  mean(obs) .  #  MonteCarloObservable.error_naive     Method .  error_naive(obs::Observable{T})  Estimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.  Strategy: just take largest R value considering an upper limit for bin size (min_nbins)  #  MonteCarloObservable.error_with_convergence     Method .  Returns one sigma error and convergence flag (boolean).  #  MonteCarloObservable.iswithinerrorbars     Method .  iswithinerrorbars(A::AbstractArray{T :Number}, B::AbstractArray{T :Number}, \u0394::AbstractArray{ :Real}[, print=false])  Elementwise check whether  A  and  B  are equal up to given real error matrix  \u0394 . Will print  A \u2248 B + K.*\u0394  for  print=true .  #  MonteCarloObservable.iswithinerrorbars     Method .  iswithinerrorbars(a, b, \u03b4[, print=false])  Checks whether numbers  a  and  b  are equal up to given error  \u03b4 . Will print  x \u2248 y + k\u00b7\u03b4  for  print=true .  Is equivalent to  isapprox(a,b,atol=\u03b4,rtol=zero(b)) .  #  MonteCarloObservable.jackknife_error     Method .  jackknife_error(g::Function, obs1, ob2, ...)  Computes the jackknife one sigma error of  g( obs1 ,  obs2 , ...)  by performing  a \"leave-one-out\" analysis.  The function  g(x)  must take one matrix argument  x , whose columns correspond  to the time series of the observables, and produce a scalar (point estimate).  Example:  g(x) = @views mean(x[:,1])^2 - mean(x[:,2].^2)  followed by  jackknife_error(g, obs1, obs2) . Here  x[:,1]  is basically  timeseries(obs1)  and  x[:,2]  is  timeseries(obs2) .  #  MonteCarloObservable.tau     Method .  tau(obs::Observable{T})  Integrated autocorrelation time (obtained by binning analysis).  See also  error(obs) .  #  Statistics.mean     Method .  mean(obs::Observable{T})  Estimate of the mean of the observable.  #  Statistics.std     Method .  std(obs::Observable{T})  Standard deviation of the time series (assuming uncorrelated data).  See also  mean(obs) ,  var(obs) , and  error(obs) .  #  Statistics.var     Method .  var(obs::Observable{T})  Variance of the time series (assuming uncorrelated data).  See also  mean(obs) ,  std(obs) , and  error(obs) .  #  MonteCarloObservable.binning_error     Method .  binning_error(X)  Estimate of the one-sigma error of the time series's mean. Respects correlations between measurements through binning analysis.  Note that this is not the same as  Base.std(X) , not even for uncorrelated measurements.  For more details, see  MonteCarloObservable.Rplateaufinder .  #  MonteCarloObservable.binning_error     Method .  binning_error(X, binsize)  Estimate of the one-sigma error of the time series's mean. Respect correlations between measurements through binning analysis,  using the given  binsize  (i.e. assuming independence of bins, Eq. 3.18 basically).", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/io/", 
            "text": "Methods: IO\n\n\nBelow you find all IO related exports.\n\n\n\n\nIndex\n\n\n\n\nMonteCarloObservable.export_error\n\n\nMonteCarloObservable.export_result\n\n\nMonteCarloObservable.listobs\n\n\nMonteCarloObservable.loadobs\n\n\nMonteCarloObservable.loadobs_frommemory\n\n\nMonteCarloObservable.rmobs\n\n\nMonteCarloObservable.saveobs\n\n\nMonteCarloObservable.timeseries_frommemory\n\n\nMonteCarloObservable.timeseries_frommemory_flat\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nMonteCarloObservable.export_error\n \n \nMethod\n.\n\n\nexport_error\n(\nobs\n::\nObservable\n{\nT\n}\n[\n,\n \nfilename\n::AbstractString\n,\n \ngroup\n::AbstractString\n;\n]\n)\n\n\n\n\n\n\nExport one-sigma error estimate and convergence flag.\n\n\n#\n\n\nMonteCarloObservable.export_result\n \n \nMethod\n.\n\n\nexport_results\n(\nobs\n::\nObservable\n{\nT\n}\n[\n,\n \nfilename\n::AbstractString\n,\n \ngroup\n::AbstractString\n;\n \ntimeseries\n::Bool\n=\nfalse\n]\n)\n\n\n\n\n\n\nExport result for given observable nicely to JLD.\n\n\nWill export name, number of measurements, estimates for mean and one-sigma error. Optionally (\ntimeseries==true\n) exports the full time series as well.\n\n\n#\n\n\nMonteCarloObservable.listobs\n \n \nFunction\n.\n\n\nList all observables in a given file and HDF5 group.\n\n\n#\n\n\nMonteCarloObservable.loadobs\n \n \nMethod\n.\n\n\nloadobs(filename::AbstractString, entryname::AbstractString)\n\n\n\n\n\nLoad complete representation of an observable from JLD file.\n\n\nSee also \nsaveobs\n.\n\n\n#\n\n\nMonteCarloObservable.loadobs_frommemory\n \n \nMethod\n.\n\n\nloadobs_frommemory(filename::AbstractString, group::AbstractString)\n\n\n\n\n\nCreate an observable based on memory dump (\ninmemory==false\n).\n\n\n#\n\n\nMonteCarloObservable.rmobs\n \n \nFunction\n.\n\n\nRemove an observable.\n\n\n#\n\n\nMonteCarloObservable.saveobs\n \n \nMethod\n.\n\n\nsaveobs(obs::Observable{T}[, filename::AbstractString, entryname::AbstractString])\n\n\n\n\n\nSaves complete representation of the observable to JLD file.\n\n\nDefault filename is \"Observables.jld\" and default entryname is \nname(obs)\n.\n\n\nSee also \nloadobs\n.\n\n\n#\n\n\nMonteCarloObservable.timeseries_frommemory\n \n \nMethod\n.\n\n\ntimeseries_frommemory(filename::AbstractString, group::AbstractString)\n\n\n\n\n\nLoad time series from memory dump (\ninmemory==false\n) in HDF5/JLD file.\n\n\nWill load and concatenate time series chunks. Output will be a vector of measurements.\n\n\n#\n\n\nMonteCarloObservable.timeseries_frommemory_flat\n \n \nMethod\n.\n\n\ntimeseries_frommemory_flat(filename::AbstractString, group::AbstractString)\n\n\n\n\n\nLoad time series from memory dump (\ninmemory==false\n) in HDF5/JLD file.\n\n\nWill load and concatenate time series chunks. Output will be higher-dimensional array whose last dimension corresponds to Monte Carlo time.", 
            "title": "IO"
        }, 
        {
            "location": "/methods/io/#methods-io", 
            "text": "Below you find all IO related exports.", 
            "title": "Methods: IO"
        }, 
        {
            "location": "/methods/io/#index", 
            "text": "MonteCarloObservable.export_error  MonteCarloObservable.export_result  MonteCarloObservable.listobs  MonteCarloObservable.loadobs  MonteCarloObservable.loadobs_frommemory  MonteCarloObservable.rmobs  MonteCarloObservable.saveobs  MonteCarloObservable.timeseries_frommemory  MonteCarloObservable.timeseries_frommemory_flat", 
            "title": "Index"
        }, 
        {
            "location": "/methods/io/#documentation", 
            "text": "#  MonteCarloObservable.export_error     Method .  export_error ( obs :: Observable { T } [ ,   filename ::AbstractString ,   group ::AbstractString ; ] )   Export one-sigma error estimate and convergence flag.  #  MonteCarloObservable.export_result     Method .  export_results ( obs :: Observable { T } [ ,   filename ::AbstractString ,   group ::AbstractString ;   timeseries ::Bool = false ] )   Export result for given observable nicely to JLD.  Will export name, number of measurements, estimates for mean and one-sigma error. Optionally ( timeseries==true ) exports the full time series as well.  #  MonteCarloObservable.listobs     Function .  List all observables in a given file and HDF5 group.  #  MonteCarloObservable.loadobs     Method .  loadobs(filename::AbstractString, entryname::AbstractString)  Load complete representation of an observable from JLD file.  See also  saveobs .  #  MonteCarloObservable.loadobs_frommemory     Method .  loadobs_frommemory(filename::AbstractString, group::AbstractString)  Create an observable based on memory dump ( inmemory==false ).  #  MonteCarloObservable.rmobs     Function .  Remove an observable.  #  MonteCarloObservable.saveobs     Method .  saveobs(obs::Observable{T}[, filename::AbstractString, entryname::AbstractString])  Saves complete representation of the observable to JLD file.  Default filename is \"Observables.jld\" and default entryname is  name(obs) .  See also  loadobs .  #  MonteCarloObservable.timeseries_frommemory     Method .  timeseries_frommemory(filename::AbstractString, group::AbstractString)  Load time series from memory dump ( inmemory==false ) in HDF5/JLD file.  Will load and concatenate time series chunks. Output will be a vector of measurements.  #  MonteCarloObservable.timeseries_frommemory_flat     Method .  timeseries_frommemory_flat(filename::AbstractString, group::AbstractString)  Load time series from memory dump ( inmemory==false ) in HDF5/JLD file.  Will load and concatenate time series chunks. Output will be higher-dimensional array whose last dimension corresponds to Monte Carlo time.", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/plotting/", 
            "text": "Methods: Statistics\n\n\nBelow you find all plotting related exports.\n\n\n\n\nIndex\n\n\n\n\nMonteCarloObservable.binningplot\n\n\nMonteCarloObservable.corrplot\n\n\nMonteCarloObservable.errorplot\n\n\nMonteCarloObservable.hist\n\n\nPyPlot.plot\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nMonteCarloObservable.binningplot\n \n \nMethod\n.\n\n\nbinningplot\n(\nobs\n::\nObservable\n{\nT\n}\n[\n;\n \nmin_nbins\n=\n32\n]\n)\n\n\n\n\n\n\nCreates a plot of the binning error coefficient \nR\n as a function of bin size.\n\n\nThe coefficient \nR\n should (up to statistical fluctuations) show a plateau for larger bin sizes, indicating that the bin averages have become independent. For correlated data one has \nR\n\u22481\n and \nsqrt(R)\n quantifies how much one would have underestimated the one-sigma errorbar.\n\n\nSee \nbinning_error\n.\n\n\n#\n\n\nMonteCarloObservable.corrplot\n \n \nMethod\n.\n\n\ncorrplot(obs::Observable{T})\n\n\n\n\n\nPlot the autocorrelation function of the observable.\n\n\n#\n\n\nMonteCarloObservable.errorplot\n \n \nMethod\n.\n\n\nerrorplot(obs::Observable{T})\n\n\n\n\n\nCreates a plot of the statistical standard error as a function of bin size.\n\n\nThe standard error should (up to statistical fluctuations) show a plateau for larger bin sizes, indicating that the bin averages have become independent.\n\n\nSee \nbinning_error\n, \nerror\n.\n\n\n#\n\n\nMonteCarloObservable.hist\n \n \nMethod\n.\n\n\nhist\n(\nobs\n::\nObservable\n{\nT\n}\n[\n;\n \nerrors\n=\ntrue\n,\n \ndigits\n=\n3\n]\n)\n\n\n\n\n\n\nPlot a histogram of the observable's time series.\n\n\n#\n\n\nPyPlot.plot\n \n \nMethod\n.\n\n\nplot\n(\nobs\n::\nObservable\n{\nT\n}\n[\n;\n \nerrors\n=\ntrue\n,\n \ndigits\n=\n3\n]\n)\n\n\n\n\n\n\nPlot the observable's time series.", 
            "title": "Plotting"
        }, 
        {
            "location": "/methods/plotting/#methods-statistics", 
            "text": "Below you find all plotting related exports.", 
            "title": "Methods: Statistics"
        }, 
        {
            "location": "/methods/plotting/#index", 
            "text": "MonteCarloObservable.binningplot  MonteCarloObservable.corrplot  MonteCarloObservable.errorplot  MonteCarloObservable.hist  PyPlot.plot", 
            "title": "Index"
        }, 
        {
            "location": "/methods/plotting/#documentation", 
            "text": "#  MonteCarloObservable.binningplot     Method .  binningplot ( obs :: Observable { T } [ ;   min_nbins = 32 ] )   Creates a plot of the binning error coefficient  R  as a function of bin size.  The coefficient  R  should (up to statistical fluctuations) show a plateau for larger bin sizes, indicating that the bin averages have become independent. For correlated data one has  R \u22481  and  sqrt(R)  quantifies how much one would have underestimated the one-sigma errorbar.  See  binning_error .  #  MonteCarloObservable.corrplot     Method .  corrplot(obs::Observable{T})  Plot the autocorrelation function of the observable.  #  MonteCarloObservable.errorplot     Method .  errorplot(obs::Observable{T})  Creates a plot of the statistical standard error as a function of bin size.  The standard error should (up to statistical fluctuations) show a plateau for larger bin sizes, indicating that the bin averages have become independent.  See  binning_error ,  error .  #  MonteCarloObservable.hist     Method .  hist ( obs :: Observable { T } [ ;   errors = true ,   digits = 3 ] )   Plot a histogram of the observable's time series.  #  PyPlot.plot     Method .  plot ( obs :: Observable { T } [ ;   errors = true ,   digits = 3 ] )   Plot the observable's time series.", 
            "title": "Documentation"
        }
    ]
}