{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThis package provides an implementation of an observable in a Markov Chain Monte Carlo simulation context (like \nMonteCarlo.jl\n).\n\n\nDuring a \nMarkov chain Monte Carlo simulation\n a Markov walker (after thermalization) walks through configuration space according to the equilibrium distribution. Typically, one measures observables along the Markov path, records the results, and in the end averages the measurements. \nMonteCarloObservable.jl\n provides all the necessary tools for conveniently conducting these types of measurements, including estimation of one-sigma error bars through binning or jackknife analysis.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "This package provides an implementation of an observable in a Markov Chain Monte Carlo simulation context (like  MonteCarlo.jl ).  During a  Markov chain Monte Carlo simulation  a Markov walker (after thermalization) walks through configuration space according to the equilibrium distribution. Typically, one measures observables along the Markov path, records the results, and in the end averages the measurements.  MonteCarloObservable.jl  provides all the necessary tools for conveniently conducting these types of measurements, including estimation of one-sigma error bars through binning or jackknife analysis.", 
            "title": "Introduction"
        }, 
        {
            "location": "/manual/gettingstarted/", 
            "text": "Manual\n\n\n\n\nInstallation / Updating\n\n\nTo install the package execute the following command in the REPL:\n\n\nPkg\n.\nclone\n(\nhttps://github.com/crstnbr/MonteCarloObservable.jl\n)\n\n\n\n\n\n\nTo obtain the latest version of the package just do \nPkg.update()\n or specifically \nPkg.update(\"MonteCarloObservable\")\n.\n\n\n\n\nExample\n\n\nThis is a simple demontration of how to use the package for measuring a floating point observable:\n\n\njulia\n using MonteCarloObservable\n\njulia\n obs = Observable(Float64, \nmyobservable\n)\nObservable \nmyobservable\n of type Float64 with 0 measurements\n\njulia\n add!(obs, 1.23) # add measurement\n\njulia\n obs\nObservable \nmyobservable\n of type  Float64 with 1 measurement\n\njulia\n push!(obs, rand(4)) # same as add!\n\njulia\n length(obs)\n5\n\njulia\n timeseries(obs)\n5-element Array{Float64,1}:\n 1.23\n 0.43811537301209236\n 0.6340456471371105\n 0.1004972346383688\n 0.7593323745564167\n\njulia\n obs[3] # conventional element accessing\n0.6340456471371105\n\njulia\n obs[end-2:end]\n3-element Array{Float64,1}:\n 0.6340456471371105\n 0.1004972346383688\n 0.7593323745564167\n\njulia\n add!(obs, rand(995))\n\njulia\n mean(obs)\nERROR: UndefVarError: mean not defined\n\njulia\n error(obs) # one-sigma error of mean (binning analysis)\n0.009167067878297746\n\njulia\n saveobs(obs, \nmyobservable.jld\n)\n\n\n\n\n\nTODO:\n mention \nalloc\n keyword and importance of preallocation.", 
            "title": "Getting started"
        }, 
        {
            "location": "/manual/gettingstarted/#manual", 
            "text": "", 
            "title": "Manual"
        }, 
        {
            "location": "/manual/gettingstarted/#installation-updating", 
            "text": "To install the package execute the following command in the REPL:  Pkg . clone ( https://github.com/crstnbr/MonteCarloObservable.jl )   To obtain the latest version of the package just do  Pkg.update()  or specifically  Pkg.update(\"MonteCarloObservable\") .", 
            "title": "Installation / Updating"
        }, 
        {
            "location": "/manual/gettingstarted/#example", 
            "text": "This is a simple demontration of how to use the package for measuring a floating point observable:  julia  using MonteCarloObservable\n\njulia  obs = Observable(Float64,  myobservable )\nObservable  myobservable  of type Float64 with 0 measurements\n\njulia  add!(obs, 1.23) # add measurement\n\njulia  obs\nObservable  myobservable  of type  Float64 with 1 measurement\n\njulia  push!(obs, rand(4)) # same as add!\n\njulia  length(obs)\n5\n\njulia  timeseries(obs)\n5-element Array{Float64,1}:\n 1.23\n 0.43811537301209236\n 0.6340456471371105\n 0.1004972346383688\n 0.7593323745564167\n\njulia  obs[3] # conventional element accessing\n0.6340456471371105\n\njulia  obs[end-2:end]\n3-element Array{Float64,1}:\n 0.6340456471371105\n 0.1004972346383688\n 0.7593323745564167\n\njulia  add!(obs, rand(995))\n\njulia  mean(obs)\nERROR: UndefVarError: mean not defined\n\njulia  error(obs) # one-sigma error of mean (binning analysis)\n0.009167067878297746\n\njulia  saveobs(obs,  myobservable.jld )  TODO:  mention  alloc  keyword and importance of preallocation.", 
            "title": "Example"
        }, 
        {
            "location": "/manual/meantype/", 
            "text": "Manual\n\n\n\n\nIssue\n\n\nLet's assume you have an observable of type \nInt\n. You want to add the following measurements to your observable:\n\n\nx\n \n=\n \nInt\n[\n44\n,\n \n-\n70\n,\n \n14\n,\n \n-\n32\n,\n \n18\n]\n\n\n\n\n\n\nThe mean of the observable should afterwards be \nmean(x) == -5.2\n. Note that this value, \n-5.2\n, exceeds the type \nInt\n. Explicitly we can see this if we try to convert it to \nInt\n:\n\n\njulia\n \nconvert\n(\nInt\n,\n \nmean\n(\nx\n))\n\n\nERROR\n:\n \nInexactError\n()\n\n\nStacktrace\n:\n\n \n[\n1\n]\n \nconvert\n(\n::\nType\n{\nInt\n},\n \n::\nFloat64\n)\n \nat\n \n.\\\nfloat\n.\njl\n:\n679\n\n\n\n\n\n\nThis is, of course, not a particularity of the type \nInt\n but happens for many (discrete) data types.\n\n\n\n\nDefault\n\n\nLet us try the above example explicitly. We create an integer observable,\n\n\njulia\n \nmyobs\n \n=\n \nObservable\n(\nInt\n,\n \nMy Observable\n);\n\n\n\n\n\n\nand add the measurements,\n\n\njulia\n \nadd!\n(\nmyobs\n,\n \nInt\n[\n44\n,\n \n-\n70\n,\n \n14\n,\n \n-\n32\n,\n \n18\n]);\n\n\n\n\n\n\nLet's see what we get for the mean\n\n\njulia\n \nmean\n(\nmyobs\n)\n\n\n-\n5.2\n\n\n\n\n\n\nSo apparently the package handles the above issue.\n\n\nHow does it do it? Basically it applies a heuristic for setting a resonable type for the mean. It creates an array (or number) of the same dimensionality as a measurement and takes, depending on wether the element type is real or complex, either the type \nFloat64\n or \nComplexF64\n as element type for the mean. For the above example we can check this, \ntypeof(mean(myobs)) == Float64\n.\n\n\n\n\nmeantype\n keyword\n\n\nThe above heuristic should be reasonable for most cases. However, if it isn't you can set the type of the mean explicitly via the keyword \nmeantype\n. Example:\n\n\njulia\n \nmyobs\n \n=\n \nObservable\n(\nInt\n,\n \nMy Observable\n,\n \nmeantype\n=\nFloat32\n);\n\n\n\njulia\n \nadd!\n(\nmyobs\n,\n \nInt\n[\n44\n,\n \n-\n70\n,\n \n14\n,\n \n-\n32\n,\n \n18\n]);\n\n\n\njulia\n \ntypeof\n(\nmean\n(\nmyobs\n))\n \n==\n \nFloat32", 
            "title": "Type of the mean"
        }, 
        {
            "location": "/manual/meantype/#manual", 
            "text": "", 
            "title": "Manual"
        }, 
        {
            "location": "/manual/meantype/#issue", 
            "text": "Let's assume you have an observable of type  Int . You want to add the following measurements to your observable:  x   =   Int [ 44 ,   - 70 ,   14 ,   - 32 ,   18 ]   The mean of the observable should afterwards be  mean(x) == -5.2 . Note that this value,  -5.2 , exceeds the type  Int . Explicitly we can see this if we try to convert it to  Int :  julia   convert ( Int ,   mean ( x ))  ERROR :   InexactError ()  Stacktrace : \n  [ 1 ]   convert ( :: Type { Int },   :: Float64 )   at   .\\ float . jl : 679   This is, of course, not a particularity of the type  Int  but happens for many (discrete) data types.", 
            "title": "Issue"
        }, 
        {
            "location": "/manual/meantype/#default", 
            "text": "Let us try the above example explicitly. We create an integer observable,  julia   myobs   =   Observable ( Int ,   My Observable );   and add the measurements,  julia   add! ( myobs ,   Int [ 44 ,   - 70 ,   14 ,   - 32 ,   18 ]);   Let's see what we get for the mean  julia   mean ( myobs )  - 5.2   So apparently the package handles the above issue.  How does it do it? Basically it applies a heuristic for setting a resonable type for the mean. It creates an array (or number) of the same dimensionality as a measurement and takes, depending on wether the element type is real or complex, either the type  Float64  or  ComplexF64  as element type for the mean. For the above example we can check this,  typeof(mean(myobs)) == Float64 .", 
            "title": "Default"
        }, 
        {
            "location": "/manual/meantype/#meantype-keyword", 
            "text": "The above heuristic should be reasonable for most cases. However, if it isn't you can set the type of the mean explicitly via the keyword  meantype . Example:  julia   myobs   =   Observable ( Int ,   My Observable ,   meantype = Float32 );  julia   add! ( myobs ,   Int [ 44 ,   - 70 ,   14 ,   - 32 ,   18 ]);  julia   typeof ( mean ( myobs ))   ==   Float32", 
            "title": "meantype keyword"
        }, 
        {
            "location": "/manual/errorestimation/", 
            "text": "Error estimation\n\n\nThe automatic estimation of error bars (one-sigma confidence intervals) is outsourced in the package \nErrorAnalysis.jl\n.\n\n\n\n\nBinning analysis\n\n\nFor \nN\nN\n uncorrelated measurements of an observable \nO\nO\n the statistical error \n\\sigma\n\\sigma\n, the root-mean-square deviation of the time series mean from the true expectation value, falls off with the number of measurements \nN\nN\n according to\n\n\n\n\n\n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},\n\n\n\n\n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},\n\n\n\n\n\nwhere \n\\sigma_{O}\n\\sigma_{O}\n is the standard deviation of the observable \nO\nO\n.\n\n\nIn a Markov chain Monte Carlo sampling, however, measurements are usually correlated due to the fact that the next step of the Markov walker depends on his current position in configuration space. One way to estimate the statistical error in this case is by binning analysis. The idea is to partition the time series into bins of a fixed size large enough such that neighboring bins are uncorrelated, that is there means are uncorrelated. For this procedure to be reliable we need both a large bin size (larger than the Markov time scale of correlations, typically called autocorrelation time) and many bins (to suppress statistical fluctuations).\n\n\nIn principle, what one \nshould\n do is look at the estimate for the statistical error as a function of bin size and expect a plateau (convergence of the estimate). However, finding a plateau (with fluctuations) numerically in an automated manner is difficult. Instead we simply always partition the time series into 32 bins what is generally considered a large sample by statisticians. The more data we add the larger the bin size and the better the error estimate.\n\n\nFrom this we conclude that estimates for the error only become reliable in the limit of many measurements.\n\n\n\n\nResources\n\n\nJ. Gubernatis, N. Kawashima, and P. Werner, \nQuantum Monte Carlo Methods: Algorithms for Lattice Models\n, Book (2016)\n\n\nV. Ambegaokar, and M. Troyer, \nEstimating errors reliably in Monte Carlo simulations of the Ehrenfest model\n, American Journal of Physics \n78\n, 150 (2010)", 
            "title": "Error estimation"
        }, 
        {
            "location": "/manual/errorestimation/#error-estimation", 
            "text": "The automatic estimation of error bars (one-sigma confidence intervals) is outsourced in the package  ErrorAnalysis.jl .", 
            "title": "Error estimation"
        }, 
        {
            "location": "/manual/errorestimation/#binning-analysis", 
            "text": "For  N N  uncorrelated measurements of an observable  O O  the statistical error  \\sigma \\sigma , the root-mean-square deviation of the time series mean from the true expectation value, falls off with the number of measurements  N N  according to   \n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},  \n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},   where  \\sigma_{O} \\sigma_{O}  is the standard deviation of the observable  O O .  In a Markov chain Monte Carlo sampling, however, measurements are usually correlated due to the fact that the next step of the Markov walker depends on his current position in configuration space. One way to estimate the statistical error in this case is by binning analysis. The idea is to partition the time series into bins of a fixed size large enough such that neighboring bins are uncorrelated, that is there means are uncorrelated. For this procedure to be reliable we need both a large bin size (larger than the Markov time scale of correlations, typically called autocorrelation time) and many bins (to suppress statistical fluctuations).  In principle, what one  should  do is look at the estimate for the statistical error as a function of bin size and expect a plateau (convergence of the estimate). However, finding a plateau (with fluctuations) numerically in an automated manner is difficult. Instead we simply always partition the time series into 32 bins what is generally considered a large sample by statisticians. The more data we add the larger the bin size and the better the error estimate.  From this we conclude that estimates for the error only become reliable in the limit of many measurements.", 
            "title": "Binning analysis"
        }, 
        {
            "location": "/manual/errorestimation/#resources", 
            "text": "J. Gubernatis, N. Kawashima, and P. Werner,  Quantum Monte Carlo Methods: Algorithms for Lattice Models , Book (2016)  V. Ambegaokar, and M. Troyer,  Estimating errors reliably in Monte Carlo simulations of the Ehrenfest model , American Journal of Physics  78 , 150 (2010)", 
            "title": "Resources"
        }, 
        {
            "location": "/manual/memdisk/", 
            "text": "Memory / disk storage\n\n\nBy default the full Monte Carlo time series of an observable is kept in memory. This is the most convenient option as it renders element access and error computation fast. However, one can think of at least two scenarios in which it might be preferable to track the time series on disk rather than in memory:\n\n\n\n\nAbrupt termination: the simulation might be computationally expensive, thus slow, and might abort abruptly (maybe due to cluster outage or time limit). In this case, one probably wants to have a restorable \"memory dump\" of the so far recorded measurements to not have to restart from scratch.\n\n\nMemory limit: the tracked observable might be large, i.e. a large complex matrix. Then, storing a long time series might make the simulation exceed a memory limit (and often stop unexpectedly). Keeping the time series memory on disk solves this problem.\n\n\n\n\nAs we show below, \nMonteCarloObservable.jl\n allows you to handle those cases by keeping the time series on disk.\n\n\n\n\nNote\n\n\nOne can always save the full observable object (\nsaveobs\n) or export the time series to disk (\nexport_result\n with \ntimeseries=true\n). This section is about the (internal) temporary storage of the time series during simulation. If you will, you can think of in-memory observables (default) and \"disk observables\" (this section).\n\n\n\n\n\n\n\"Disk observables\"\n\n\nYou can create a \"disk observable\" that every once in a while dumps it's time series memory to disk as follows:\n\n\nobs\n \n=\n \nObservable\n(\nFloat64\n,\n \nmyobservable\n;\n \ninmemory\n=\nfalse\n,\n \nalloc\n=\n100\n)\n\n\n\n\n\n\nIt will record measurements in memory until the preallocated time series buffer (\nalloc=100\n) overflows in which case it will save the observables memory to JLD file (\noutfile\n). In the above example this will thus happen for the first time after 100 measurements.\n\n\nApart from the special initialization (\ninmemory=false\n) basically everything else stays the same as for an in-memory observable. For example, we can still get the mean via \nmean(obs)\n, access time series elements with \nobs[idx]\n and load the full time series to memory at any point via \ntimeseries(obs)\n. However, because of now necessary disk operations same functionality might be slightly slower for those \"disk observables\".\n\n\nThe observable's memory dump contains meta information, like name, element type, element size etc., as well as time series memory chunks. The dumping is implemented in the not exported method \nMonteCarloObservable.updateondisk\n. Note that the observable's memory is \nnot\n a full backup of the observable itself (see \nsaveobs\n). Should the simulation terminate abruptly one can nonetheless restore most of the so-far recorded information using \nloadobs_frommemory\n and \ntimeseries_frommemory\n. Measurements that haven't been dumped yet, because they are still lying in the preallocated buffer, are lost though. Please also note that the structure of the dump of an observable's memory might change in future versions.", 
            "title": "Memory / disk storage"
        }, 
        {
            "location": "/manual/memdisk/#memory-disk-storage", 
            "text": "By default the full Monte Carlo time series of an observable is kept in memory. This is the most convenient option as it renders element access and error computation fast. However, one can think of at least two scenarios in which it might be preferable to track the time series on disk rather than in memory:   Abrupt termination: the simulation might be computationally expensive, thus slow, and might abort abruptly (maybe due to cluster outage or time limit). In this case, one probably wants to have a restorable \"memory dump\" of the so far recorded measurements to not have to restart from scratch.  Memory limit: the tracked observable might be large, i.e. a large complex matrix. Then, storing a long time series might make the simulation exceed a memory limit (and often stop unexpectedly). Keeping the time series memory on disk solves this problem.   As we show below,  MonteCarloObservable.jl  allows you to handle those cases by keeping the time series on disk.   Note  One can always save the full observable object ( saveobs ) or export the time series to disk ( export_result  with  timeseries=true ). This section is about the (internal) temporary storage of the time series during simulation. If you will, you can think of in-memory observables (default) and \"disk observables\" (this section).", 
            "title": "Memory / disk storage"
        }, 
        {
            "location": "/manual/memdisk/#disk-observables", 
            "text": "You can create a \"disk observable\" that every once in a while dumps it's time series memory to disk as follows:  obs   =   Observable ( Float64 ,   myobservable ;   inmemory = false ,   alloc = 100 )   It will record measurements in memory until the preallocated time series buffer ( alloc=100 ) overflows in which case it will save the observables memory to JLD file ( outfile ). In the above example this will thus happen for the first time after 100 measurements.  Apart from the special initialization ( inmemory=false ) basically everything else stays the same as for an in-memory observable. For example, we can still get the mean via  mean(obs) , access time series elements with  obs[idx]  and load the full time series to memory at any point via  timeseries(obs) . However, because of now necessary disk operations same functionality might be slightly slower for those \"disk observables\".  The observable's memory dump contains meta information, like name, element type, element size etc., as well as time series memory chunks. The dumping is implemented in the not exported method  MonteCarloObservable.updateondisk . Note that the observable's memory is  not  a full backup of the observable itself (see  saveobs ). Should the simulation terminate abruptly one can nonetheless restore most of the so-far recorded information using  loadobs_frommemory  and  timeseries_frommemory . Measurements that haven't been dumped yet, because they are still lying in the preallocated buffer, are lost though. Please also note that the structure of the dump of an observable's memory might change in future versions.", 
            "title": "\"Disk observables\""
        }, 
        {
            "location": "/methods/general/", 
            "text": "Methods: General\n\n\nBelow you find all general exports.\n\n\n\n\nIndex\n\n\n\n\nMonteCarloObservable.Observable\n\n\nMonteCarloObservable.add!\n\n\nMonteCarloObservable.add!\n\n\nMonteCarloObservable.inmemory\n\n\nMonteCarloObservable.name\n\n\nMonteCarloObservable.rename\n\n\nMonteCarloObservable.reset!\n\n\nMonteCarloObservable.timeseries\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nMonteCarloObservable.Observable\n \n \nMethod\n.\n\n\nObservable(t, name; keyargs...)\n\n\n\n\n\nCreate an observable of type \nt\n.\n\n\nThe following keywords are allowed:\n\n\n\n\nalloc\n: preallocated size of time series container\n\n\noutfile\n: default HDF5/JLD output file for io operations\n\n\ndataset\n: target path within \noutfile\n\n\ninmemory\n: wether to keep the time series in memory or on disk\n\n\nmeantype\n: type of the mean (should be compatible with measurement type \nt\n)\n\n\n\n\nSee also \nObservable\n.\n\n\n#\n\n\nMonteCarloObservable.add!\n \n \nMethod\n.\n\n\nadd\n!(\nobs\n::\nObservable\n{\nT\n}\n,\n \nmeasurements\n::\nAbstractArray\n{\nT\n}\n;\n \nverbose\n=\nfalse\n)\n\n\n\n\n\n\nAdd multiple \nmeasurements\n to observable \nobs\n.\n\n\n#\n\n\nMonteCarloObservable.add!\n \n \nMethod\n.\n\n\nadd\n!(\nobs\n::\nObservable\n{\nT\n}\n,\n \nmeasurement\n::\nT\n;\n \nverbose\n=\nfalse\n)\n\n\n\n\n\n\nAdd a \nmeasurement\n to observable \nobs\n.\n\n\n#\n\n\nMonteCarloObservable.inmemory\n \n \nMethod\n.\n\n\ninmemory(obs::Observable{T})\n\n\n\n\n\nChecks wether the observable is kept in memory (vs. on disk).\n\n\n#\n\n\nMonteCarloObservable.name\n \n \nMethod\n.\n\n\nname(obs::Observable{T})\n\n\n\n\n\nReturns the name of the observable.\n\n\n#\n\n\nMonteCarloObservable.rename\n \n \nMethod\n.\n\n\nrename(obs::Observable{T}, name)\n\n\n\n\n\nRenames the observable.\n\n\n#\n\n\nMonteCarloObservable.reset!\n \n \nMethod\n.\n\n\nreset!(obs::Observable{T})\n\n\n\n\n\nResets all measurement information in \nobs\n. Identical to \ninit!\n and \nclear!\n.\n\n\n#\n\n\nMonteCarloObservable.timeseries\n \n \nMethod\n.\n\n\ntimeseries(obs::Observable{T})\n\n\n\n\n\nReturns the measurement time series of an observable.\n\n\nIf \ninmemory(obs) == false\n it will read the time series from disk and thus might take some time.\n\n\nSee also \ngetindex\n and \nview\n.", 
            "title": "General"
        }, 
        {
            "location": "/methods/general/#methods-general", 
            "text": "Below you find all general exports.", 
            "title": "Methods: General"
        }, 
        {
            "location": "/methods/general/#index", 
            "text": "MonteCarloObservable.Observable  MonteCarloObservable.add!  MonteCarloObservable.add!  MonteCarloObservable.inmemory  MonteCarloObservable.name  MonteCarloObservable.rename  MonteCarloObservable.reset!  MonteCarloObservable.timeseries", 
            "title": "Index"
        }, 
        {
            "location": "/methods/general/#documentation", 
            "text": "#  MonteCarloObservable.Observable     Method .  Observable(t, name; keyargs...)  Create an observable of type  t .  The following keywords are allowed:   alloc : preallocated size of time series container  outfile : default HDF5/JLD output file for io operations  dataset : target path within  outfile  inmemory : wether to keep the time series in memory or on disk  meantype : type of the mean (should be compatible with measurement type  t )   See also  Observable .  #  MonteCarloObservable.add!     Method .  add !( obs :: Observable { T } ,   measurements :: AbstractArray { T } ;   verbose = false )   Add multiple  measurements  to observable  obs .  #  MonteCarloObservable.add!     Method .  add !( obs :: Observable { T } ,   measurement :: T ;   verbose = false )   Add a  measurement  to observable  obs .  #  MonteCarloObservable.inmemory     Method .  inmemory(obs::Observable{T})  Checks wether the observable is kept in memory (vs. on disk).  #  MonteCarloObservable.name     Method .  name(obs::Observable{T})  Returns the name of the observable.  #  MonteCarloObservable.rename     Method .  rename(obs::Observable{T}, name)  Renames the observable.  #  MonteCarloObservable.reset!     Method .  reset!(obs::Observable{T})  Resets all measurement information in  obs . Identical to  init!  and  clear! .  #  MonteCarloObservable.timeseries     Method .  timeseries(obs::Observable{T})  Returns the measurement time series of an observable.  If  inmemory(obs) == false  it will read the time series from disk and thus might take some time.  See also  getindex  and  view .", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/statistics/", 
            "text": "Methods: Statistics\n\n\nBelow you find all statistics related exports.\n\n\n\n\nIndex\n\n\n\n\nBase.error\n\n\nMonteCarloObservable.error_naive\n\n\nMonteCarloObservable.error_with_convergence\n\n\nMonteCarloObservable.iswithinerrorbars\n\n\nMonteCarloObservable.iswithinerrorbars\n\n\nMonteCarloObservable.jackknife_error\n\n\nMonteCarloObservable.tau\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nBase.error\n \n \nMethod\n.\n\n\nerror(obs::Observable{T})\n\n\n\n\n\nEstimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.\n\n\nNote that this is not the same as \nBase.std(timeseries(obs))\n, not even for uncorrelated measurements.\n\n\nSee also \nmean(obs)\n.\n\n\n#\n\n\nMonteCarloObservable.error_naive\n \n \nMethod\n.\n\n\nerror_naive(obs::Observable{T})\n\n\n\n\n\nEstimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.\n\n\nStrategy: just take largest R value considering an upper limit for bin size (min_nbins)\n\n\n#\n\n\nMonteCarloObservable.error_with_convergence\n \n \nMethod\n.\n\n\nReturns one sigma error and convergence flag (boolean).\n\n\n#\n\n\nMonteCarloObservable.iswithinerrorbars\n \n \nMethod\n.\n\n\niswithinerrorbars(A::AbstractArray{T\n:Number}, B::AbstractArray{T\n:Number}, \u0394::AbstractArray{\n:Real}[, print=false])\n\n\n\n\n\nElementwise check whether \nA\n and \nB\n are equal up to given real error matrix \n\u0394\n. Will print \nA \u2248 B + K.*\u0394\n for \nprint=true\n.\n\n\n#\n\n\nMonteCarloObservable.iswithinerrorbars\n \n \nMethod\n.\n\n\niswithinerrorbars(a, b, \u03b4[, print=false])\n\n\n\n\n\nChecks whether numbers \na\n and \nb\n are equal up to given error \n\u03b4\n. Will print \nx \u2248 y + k\u00b7\u03b4\n for \nprint=true\n.\n\n\nIs equivalent to \nisapprox(a,b,atol=\u03b4,rtol=zero(b))\n.\n\n\n#\n\n\nMonteCarloObservable.jackknife_error\n \n \nMethod\n.\n\n\njackknife_error(g::Function, obs1, ob2, ...)\n\n\n\n\n\nComputes the jackknife one sigma error of \ng(\nobs1\n, \nobs2\n, ...)\n by performing  a \"leave-one-out\" analysis.\n\n\nThe function \ng(x)\n must take one matrix argument \nx\n, whose columns correspond  to the time series of the observables, and produce a scalar (point estimate).\n\n\nExample:\n\n\ng(x) = @views mean(x[:,1])^2 - mean(x[:,2].^2)\n followed by \njackknife_error(g, obs1, obs2)\n. Here \nx[:,1]\n is basically \ntimeseries(obs1)\n and \nx[:,2]\n is \ntimeseries(obs2)\n.\n\n\n#\n\n\nMonteCarloObservable.tau\n \n \nMethod\n.\n\n\ntau(obs::Observable{T})\n\n\n\n\n\nIntegrated autocorrelation time (obtained by binning analysis).\n\n\nSee also \nerror(obs)\n.", 
            "title": "Statistics"
        }, 
        {
            "location": "/methods/statistics/#methods-statistics", 
            "text": "Below you find all statistics related exports.", 
            "title": "Methods: Statistics"
        }, 
        {
            "location": "/methods/statistics/#index", 
            "text": "Base.error  MonteCarloObservable.error_naive  MonteCarloObservable.error_with_convergence  MonteCarloObservable.iswithinerrorbars  MonteCarloObservable.iswithinerrorbars  MonteCarloObservable.jackknife_error  MonteCarloObservable.tau", 
            "title": "Index"
        }, 
        {
            "location": "/methods/statistics/#documentation", 
            "text": "#  Base.error     Method .  error(obs::Observable{T})  Estimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.  Note that this is not the same as  Base.std(timeseries(obs)) , not even for uncorrelated measurements.  See also  mean(obs) .  #  MonteCarloObservable.error_naive     Method .  error_naive(obs::Observable{T})  Estimate of the one-sigma error of the observable's mean. Respects correlations between measurements through binning analysis.  Strategy: just take largest R value considering an upper limit for bin size (min_nbins)  #  MonteCarloObservable.error_with_convergence     Method .  Returns one sigma error and convergence flag (boolean).  #  MonteCarloObservable.iswithinerrorbars     Method .  iswithinerrorbars(A::AbstractArray{T :Number}, B::AbstractArray{T :Number}, \u0394::AbstractArray{ :Real}[, print=false])  Elementwise check whether  A  and  B  are equal up to given real error matrix  \u0394 . Will print  A \u2248 B + K.*\u0394  for  print=true .  #  MonteCarloObservable.iswithinerrorbars     Method .  iswithinerrorbars(a, b, \u03b4[, print=false])  Checks whether numbers  a  and  b  are equal up to given error  \u03b4 . Will print  x \u2248 y + k\u00b7\u03b4  for  print=true .  Is equivalent to  isapprox(a,b,atol=\u03b4,rtol=zero(b)) .  #  MonteCarloObservable.jackknife_error     Method .  jackknife_error(g::Function, obs1, ob2, ...)  Computes the jackknife one sigma error of  g( obs1 ,  obs2 , ...)  by performing  a \"leave-one-out\" analysis.  The function  g(x)  must take one matrix argument  x , whose columns correspond  to the time series of the observables, and produce a scalar (point estimate).  Example:  g(x) = @views mean(x[:,1])^2 - mean(x[:,2].^2)  followed by  jackknife_error(g, obs1, obs2) . Here  x[:,1]  is basically  timeseries(obs1)  and  x[:,2]  is  timeseries(obs2) .  #  MonteCarloObservable.tau     Method .  tau(obs::Observable{T})  Integrated autocorrelation time (obtained by binning analysis).  See also  error(obs) .", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/io/", 
            "text": "Methods: IO\n\n\nBelow you find all IO related exports.\n\n\n\n\nIndex\n\n\n\n\nMonteCarloObservable.export_result\n\n\nMonteCarloObservable.loadobs\n\n\nMonteCarloObservable.loadobs_frommemory\n\n\nMonteCarloObservable.saveobs\n\n\nMonteCarloObservable.timeseries_frommemory\n\n\nMonteCarloObservable.timeseries_frommemory_flat\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nMonteCarloObservable.export_result\n \n \nMethod\n.\n\n\nexport_results\n(\nobs\n::\nObservable\n{\nT\n}\n[\n,\n \nfilename\n::AbstractString\n,\n \ngroup\n::AbstractString\n;\n \ntimeseries\n::Bool\n=\nfalse\n]\n)\n\n\n\n\n\n\nExport result for given observable nicely to JLD.\n\n\nWill export name, number of measurements, estimates for mean and one-sigma error. Optionally (\ntimeseries==true\n) exports the full time series as well.\n\n\n#\n\n\nMonteCarloObservable.loadobs\n \n \nMethod\n.\n\n\nloadobs(filename::AbstractString, entryname::AbstractString)\n\n\n\n\n\nLoad complete representation of an observable from JLD file.\n\n\nSee also \nsaveobs\n.\n\n\n#\n\n\nMonteCarloObservable.loadobs_frommemory\n \n \nMethod\n.\n\n\nloadobs_frommemory(filename::AbstractString, group::AbstractString)\n\n\n\n\n\nCreate an observable based on memory dump (\ninmemory==false\n).\n\n\n#\n\n\nMonteCarloObservable.saveobs\n \n \nMethod\n.\n\n\nsaveobs(obs::Observable{T}[, filename::AbstractString, entryname::AbstractString])\n\n\n\n\n\nSaves complete representation of the observable to JLD file.\n\n\nDefault filename is \"Observables.jld\" and default entryname is \nname(obs)\n.\n\n\nSee also \nloadobs\n.\n\n\n#\n\n\nMonteCarloObservable.timeseries_frommemory\n \n \nMethod\n.\n\n\ntimeseries_frommemory(filename::AbstractString, group::AbstractString)\n\n\n\n\n\nLoad time series from memory dump (\ninmemory==false\n) in HDF5/JLD file.\n\n\nWill load and concatenate time series chunks. Output will be a vector of measurements.\n\n\n#\n\n\nMonteCarloObservable.timeseries_frommemory_flat\n \n \nMethod\n.\n\n\ntimeseries_frommemory_flat(filename::AbstractString, group::AbstractString)\n\n\n\n\n\nLoad time series from memory dump (\ninmemory==false\n) in HDF5/JLD file.\n\n\nWill load and concatenate time series chunks. Output will be higher-dimensional array whose last dimension corresponds to Monte Carlo time.", 
            "title": "IO"
        }, 
        {
            "location": "/methods/io/#methods-io", 
            "text": "Below you find all IO related exports.", 
            "title": "Methods: IO"
        }, 
        {
            "location": "/methods/io/#index", 
            "text": "MonteCarloObservable.export_result  MonteCarloObservable.loadobs  MonteCarloObservable.loadobs_frommemory  MonteCarloObservable.saveobs  MonteCarloObservable.timeseries_frommemory  MonteCarloObservable.timeseries_frommemory_flat", 
            "title": "Index"
        }, 
        {
            "location": "/methods/io/#documentation", 
            "text": "#  MonteCarloObservable.export_result     Method .  export_results ( obs :: Observable { T } [ ,   filename ::AbstractString ,   group ::AbstractString ;   timeseries ::Bool = false ] )   Export result for given observable nicely to JLD.  Will export name, number of measurements, estimates for mean and one-sigma error. Optionally ( timeseries==true ) exports the full time series as well.  #  MonteCarloObservable.loadobs     Method .  loadobs(filename::AbstractString, entryname::AbstractString)  Load complete representation of an observable from JLD file.  See also  saveobs .  #  MonteCarloObservable.loadobs_frommemory     Method .  loadobs_frommemory(filename::AbstractString, group::AbstractString)  Create an observable based on memory dump ( inmemory==false ).  #  MonteCarloObservable.saveobs     Method .  saveobs(obs::Observable{T}[, filename::AbstractString, entryname::AbstractString])  Saves complete representation of the observable to JLD file.  Default filename is \"Observables.jld\" and default entryname is  name(obs) .  See also  loadobs .  #  MonteCarloObservable.timeseries_frommemory     Method .  timeseries_frommemory(filename::AbstractString, group::AbstractString)  Load time series from memory dump ( inmemory==false ) in HDF5/JLD file.  Will load and concatenate time series chunks. Output will be a vector of measurements.  #  MonteCarloObservable.timeseries_frommemory_flat     Method .  timeseries_frommemory_flat(filename::AbstractString, group::AbstractString)  Load time series from memory dump ( inmemory==false ) in HDF5/JLD file.  Will load and concatenate time series chunks. Output will be higher-dimensional array whose last dimension corresponds to Monte Carlo time.", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/plotting/", 
            "text": "Methods: Statistics\n\n\nBelow you find all plotting related exports.\n\n\n\n\nIndex\n\n\n\n\nMonteCarloObservable.binningplot\n\n\nMonteCarloObservable.corrplot\n\n\nMonteCarloObservable.hist\n\n\nPyPlot.plot\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nMonteCarloObservable.binningplot\n \n \nMethod\n.\n\n\nbinningplot\n(\nobs\n::\nObservable\n{\nT\n}\n[\n;\n \nmin_nbins\n=\n32\n]\n)\n\n\n\n\n\n\nCreates a plot of the binning error coefficient \nR\n as a function of bin size.\n\n\nThe coefficient \nR\n should (up to statistical fluctuations) show a plateau for larger bin sizes, indicating that the bin averages have become independent. For correlated data one has \nR\n\u22481\n and \nsqrt(R)\n quantifies how much one would have underestimated the one-sigma errorbar.\n\n\nSee \nbinning_error\n.\n\n\n#\n\n\nMonteCarloObservable.corrplot\n \n \nMethod\n.\n\n\ncorrplot(obs::Observable{T})\n\n\n\n\n\nPlot the autocorrelation function of the observable.\n\n\n#\n\n\nMonteCarloObservable.hist\n \n \nMethod\n.\n\n\nhist\n(\nobs\n::\nObservable\n{\nT\n}\n[\n;\n \nerrors\n=\ntrue\n,\n \ndigits\n=\n3\n]\n)\n\n\n\n\n\n\nPlot a histogram of the observable's time series.\n\n\n#\n\n\nPyPlot.plot\n \n \nMethod\n.\n\n\nplot\n(\nobs\n::\nObservable\n{\nT\n}\n[\n;\n \nerrors\n=\ntrue\n,\n \ndigits\n=\n3\n]\n)\n\n\n\n\n\n\nPlot the observable's time series.", 
            "title": "Plotting"
        }, 
        {
            "location": "/methods/plotting/#methods-statistics", 
            "text": "Below you find all plotting related exports.", 
            "title": "Methods: Statistics"
        }, 
        {
            "location": "/methods/plotting/#index", 
            "text": "MonteCarloObservable.binningplot  MonteCarloObservable.corrplot  MonteCarloObservable.hist  PyPlot.plot", 
            "title": "Index"
        }, 
        {
            "location": "/methods/plotting/#documentation", 
            "text": "#  MonteCarloObservable.binningplot     Method .  binningplot ( obs :: Observable { T } [ ;   min_nbins = 32 ] )   Creates a plot of the binning error coefficient  R  as a function of bin size.  The coefficient  R  should (up to statistical fluctuations) show a plateau for larger bin sizes, indicating that the bin averages have become independent. For correlated data one has  R \u22481  and  sqrt(R)  quantifies how much one would have underestimated the one-sigma errorbar.  See  binning_error .  #  MonteCarloObservable.corrplot     Method .  corrplot(obs::Observable{T})  Plot the autocorrelation function of the observable.  #  MonteCarloObservable.hist     Method .  hist ( obs :: Observable { T } [ ;   errors = true ,   digits = 3 ] )   Plot a histogram of the observable's time series.  #  PyPlot.plot     Method .  plot ( obs :: Observable { T } [ ;   errors = true ,   digits = 3 ] )   Plot the observable's time series.", 
            "title": "Documentation"
        }
    ]
}