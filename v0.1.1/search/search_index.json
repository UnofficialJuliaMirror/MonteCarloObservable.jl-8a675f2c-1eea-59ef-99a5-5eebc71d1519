{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThis package provides an implementation of an observable for Markov Chain Monte Carlo simulations (like the currently out-dated \nMonteCarlo.jl\n).\n\n\nDuring a \nMarkov chain Monte Carlo simulation\n a Markov walker (after thermalization) walks through configuration space according to the equilibrium distribution. Typically, one measures observables along the Markov path, records the results, and in the end averages the measurements. \nMonteCarloObservable.jl\n provides all the necessary tools for conveniently conducting these types of measurements, including estimation of one-sigma error bars through binning or jackknife analysis.\n\n\n\n\nInstallation\n\n\nIn the REPL, switch to pkg mode (by pressing \n]\n) and enter\n\n\nadd\n \nhttps\n://\ngithub\n.\ncom\n/\ncrstnbr\n/\nMonteCarloObservable\n.\njl\n\n\n\n\n\n\nAlternatively, you can install the package per\n\n\nusing\n \nPkg\n\n\nPkg\n.\nadd\n(\nhttps://github.com/crstnbr/MonteCarloObservable.jl\n)", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "This package provides an implementation of an observable for Markov Chain Monte Carlo simulations (like the currently out-dated  MonteCarlo.jl ).  During a  Markov chain Monte Carlo simulation  a Markov walker (after thermalization) walks through configuration space according to the equilibrium distribution. Typically, one measures observables along the Markov path, records the results, and in the end averages the measurements.  MonteCarloObservable.jl  provides all the necessary tools for conveniently conducting these types of measurements, including estimation of one-sigma error bars through binning or jackknife analysis.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#installation", 
            "text": "In the REPL, switch to pkg mode (by pressing  ] ) and enter  add   https :// github . com / crstnbr / MonteCarloObservable . jl   Alternatively, you can install the package per  using   Pkg  Pkg . add ( https://github.com/crstnbr/MonteCarloObservable.jl )", 
            "title": "Installation"
        }, 
        {
            "location": "/manual/gettingstarted/", 
            "text": "Getting started\n\n\nSometimes an example says more than a thousand words. So let's start with one.\n\n\n\n\nSimple Example\n\n\nThis is a basic demontration of how to use the package:\n\n\njulia\n using MonteCarloObservable\n\njulia\n obs = Observable(Float64, \nmyobservable\n)\nFloat64 Observable\n\njulia\n add!(obs, 1.23) # add measurement\n\njulia\n obs\nFloat64 Observable\n\njulia\n push!(obs, rand(4)) # same as add!\n\njulia\n length(obs)\n5\n\njulia\n timeseries(obs)\n5-element Array{Float64,1}:\n 1.23\n 0.6554939874990802\n 0.01121513020970033\n 0.2671410410753714\n 0.23860447591704603\n\njulia\n obs[3] # conventional element accessing\n0.01121513020970033\n\njulia\n obs[end-2:end]\n3-element Array{Float64,1}:\n 0.01121513020970033\n 0.2671410410753714\n 0.23860447591704603\n\njulia\n add!(obs, rand(995))\n\njulia\n mean(obs)\n0.49341564583130043\n\njulia\n error(obs) # one-sigma error of mean (binning analysis)\n0.009620340401164915\n\njulia\n saveobs(obs, \nmyobservable.jld\n)\n\n\n\n\n\n\n\nCreating \nObservable\ns\n\n\nTODO:\n mention all important keywords \nTODO:\n mention \nalloc\n keyword and importance of preallocation. \nTODO:\n mention \n@obs\n and \n@diskobs\n macros", 
            "title": "Getting started"
        }, 
        {
            "location": "/manual/gettingstarted/#getting-started", 
            "text": "Sometimes an example says more than a thousand words. So let's start with one.", 
            "title": "Getting started"
        }, 
        {
            "location": "/manual/gettingstarted/#simple-example", 
            "text": "This is a basic demontration of how to use the package:  julia  using MonteCarloObservable\n\njulia  obs = Observable(Float64,  myobservable )\nFloat64 Observable\n\njulia  add!(obs, 1.23) # add measurement\n\njulia  obs\nFloat64 Observable\n\njulia  push!(obs, rand(4)) # same as add!\n\njulia  length(obs)\n5\n\njulia  timeseries(obs)\n5-element Array{Float64,1}:\n 1.23\n 0.6554939874990802\n 0.01121513020970033\n 0.2671410410753714\n 0.23860447591704603\n\njulia  obs[3] # conventional element accessing\n0.01121513020970033\n\njulia  obs[end-2:end]\n3-element Array{Float64,1}:\n 0.01121513020970033\n 0.2671410410753714\n 0.23860447591704603\n\njulia  add!(obs, rand(995))\n\njulia  mean(obs)\n0.49341564583130043\n\njulia  error(obs) # one-sigma error of mean (binning analysis)\n0.009620340401164915\n\njulia  saveobs(obs,  myobservable.jld )", 
            "title": "Simple Example"
        }, 
        {
            "location": "/manual/gettingstarted/#creating-observables", 
            "text": "TODO:  mention all important keywords  TODO:  mention  alloc  keyword and importance of preallocation.  TODO:  mention  @obs  and  @diskobs  macros", 
            "title": "Creating Observables"
        }, 
        {
            "location": "/manual/errorestimation/", 
            "text": "Error estimation\n\n\nAutomatic estimation of the \nstandard error of the mean\n (one-sigma error bars) is based on a binning analysis.\n\n\n\n\nStandard error versus standard deviation\n\n\nBe careful not to confuse the terms \"standard error (of the mean)\" and \"standard deviation\". Quoting \nWikipedia\n on this:\n\n\n\n\nPut simply, the \nstandard error\n of the sample mean is an estimate of how far the sample mean is likely to be from the population mean, whereas the \nstandard deviation\n of the sample is the degree to which individuals within the sample differ from the sample mean. If the population \nstandard deviation\n is finite, the \nstandard error\n of the mean of the sample will tend to zero with increasing sample size, because the estimate of the population mean will improve, while the \nstandard deviation\n of the sample will tend to approximate the population \nstandard deviation\n as the sample size increases.\n\n\n\n\nThe standard error of an observable can be obtained by \nerror(obs)\n and the standard deviation by \nstd(obs)\n.\n\n\n\n\n\n\nBinning analysis\n\n\nFor \nN\nN\n uncorrelated measurements of an observable \nO\nO\n the statistical standard error \n\\sigma\n\\sigma\n, the root-mean-square deviation of the time series mean from the true mean, falls off with the number of measurements \nN\nN\n according to\n\n\n\n\n\n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},\n\n\n\n\n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},\n\n\n\n\n\nwhere \n\\sigma_{O}\n\\sigma_{O}\n is the standard deviation of the observable \nO\nO\n.\n\n\nIn a Markov chain Monte Carlo sampling, however, measurements are usually correlated due to the fact that the next step of the Markov walker depends on his current position in configuration space. One way to estimate the statistical error in this case is by binning analysis. The idea is to partition the time series into bins of a fixed size large enough such that neighboring bins are uncorrelated, that is there means are uncorrelated. For this procedure to be reliable we need both a large bin size (larger than the Markov time scale of correlations, typically called autocorrelation time) and many bins (to suppress statistical fluctuations).\n\n\nThe typical procedure is to look at the estimate for the statistical error as a function of bin size and expect a plateau (convergence of the estimate). You can do this manually using \nerrorplot(obs)\n. Automatically, the package uses a plateau-finder algorithm to check wether convergence has been reached. Note, however, that finding a plateau (with expected fluctuations) numerically in an automated manner isn't trivial. Hence, the algorithm is somewhat heuristic as it is in other software like \nALPS\n and shouldn't be trusted without further manual checking. It is conservative though in the sense that it tends to be rather false-negative than false-positive.\n\n\nFrom this we conclude that estimates for the error really only become reliable in the limit of many measurements.\n\n\n\n\nReferences\n\n\nJ. Gubernatis, N. Kawashima, and P. Werner, \nQuantum Monte Carlo Methods: Algorithms for Lattice Models\n, Book (2016)\n\n\nV. Ambegaokar, and M. Troyer, \nEstimating errors reliably in Monte Carlo simulations of the Ehrenfest model\n, American Journal of Physics \n78\n, 150 (2010)\n\n\n\n\nJackknife analysis\n\n\nSee for example the corresponding \nWikipedia article\n.", 
            "title": "Error estimation"
        }, 
        {
            "location": "/manual/errorestimation/#error-estimation", 
            "text": "Automatic estimation of the  standard error of the mean  (one-sigma error bars) is based on a binning analysis.   Standard error versus standard deviation  Be careful not to confuse the terms \"standard error (of the mean)\" and \"standard deviation\". Quoting  Wikipedia  on this:   Put simply, the  standard error  of the sample mean is an estimate of how far the sample mean is likely to be from the population mean, whereas the  standard deviation  of the sample is the degree to which individuals within the sample differ from the sample mean. If the population  standard deviation  is finite, the  standard error  of the mean of the sample will tend to zero with increasing sample size, because the estimate of the population mean will improve, while the  standard deviation  of the sample will tend to approximate the population  standard deviation  as the sample size increases.   The standard error of an observable can be obtained by  error(obs)  and the standard deviation by  std(obs) .", 
            "title": "Error estimation"
        }, 
        {
            "location": "/manual/errorestimation/#binning-analysis", 
            "text": "For  N N  uncorrelated measurements of an observable  O O  the statistical standard error  \\sigma \\sigma , the root-mean-square deviation of the time series mean from the true mean, falls off with the number of measurements  N N  according to   \n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},  \n\\sigma = \\frac{\\sigma_{O}}{\\sqrt{N}},   where  \\sigma_{O} \\sigma_{O}  is the standard deviation of the observable  O O .  In a Markov chain Monte Carlo sampling, however, measurements are usually correlated due to the fact that the next step of the Markov walker depends on his current position in configuration space. One way to estimate the statistical error in this case is by binning analysis. The idea is to partition the time series into bins of a fixed size large enough such that neighboring bins are uncorrelated, that is there means are uncorrelated. For this procedure to be reliable we need both a large bin size (larger than the Markov time scale of correlations, typically called autocorrelation time) and many bins (to suppress statistical fluctuations).  The typical procedure is to look at the estimate for the statistical error as a function of bin size and expect a plateau (convergence of the estimate). You can do this manually using  errorplot(obs) . Automatically, the package uses a plateau-finder algorithm to check wether convergence has been reached. Note, however, that finding a plateau (with expected fluctuations) numerically in an automated manner isn't trivial. Hence, the algorithm is somewhat heuristic as it is in other software like  ALPS  and shouldn't be trusted without further manual checking. It is conservative though in the sense that it tends to be rather false-negative than false-positive.  From this we conclude that estimates for the error really only become reliable in the limit of many measurements.", 
            "title": "Binning analysis"
        }, 
        {
            "location": "/manual/errorestimation/#references", 
            "text": "J. Gubernatis, N. Kawashima, and P. Werner,  Quantum Monte Carlo Methods: Algorithms for Lattice Models , Book (2016)  V. Ambegaokar, and M. Troyer,  Estimating errors reliably in Monte Carlo simulations of the Ehrenfest model , American Journal of Physics  78 , 150 (2010)", 
            "title": "References"
        }, 
        {
            "location": "/manual/errorestimation/#jackknife-analysis", 
            "text": "See for example the corresponding  Wikipedia article .", 
            "title": "Jackknife analysis"
        }, 
        {
            "location": "/manual/memdisk/", 
            "text": "Memory / disk storage\n\n\nBy default the full time series of an observable is kept in memory. This is the most convenient option as it renders element access and error computation fast. However, sometimes it is preferable to track the time series on disk rather than completely in memory:\n\n\n\n\nAbrupt termination: the simulation might be computationally expensive, thus slow, and might abort abruptly (maybe due to cluster outage or time limit). In this case, one probably wants to have a restorable \"memory dump\" of the so far recorded measurements to not have to restart from scratch.\n\n\nMemory limit: the tracked observable might be large, i.e. a large complex matrix. Then, storing a long time series might make the simulation exceed a memory limit (and often stop unexpectedly). Keeping the time series memory on disk solves this problem.\n\n\n\n\n\n\n\"Disk observables\"\n\n\nA \"disk observable\" is an \nObservable\n that every once in a while dumps it's time series memory to disk and only keeps the latest data points in memory. You can create a \"disk observable\" as\n\n\nobs\n \n=\n \nObservable\n(\nFloat64\n,\n \nmyobservable\n;\n \ninmemory\n=\nfalse\n,\n \nalloc\n=\n100\n)\n\n\n\n\n\n\nIt will record measurements in memory until the preallocated time series buffer (\nalloc=100\n) overflows in which case it saves it's time series memory to disk (\noutfile\n). In the above example this will happen after 100 measurements.\n\n\nApart from the special construction (\ninmemory=false\n) everything else stays the same as for default in-memory observables. For example, we can still get the mean via \nmean(obs)\n, access time series elements with \nobs[idx]\n and load the full time series to memory at any point through \ntimeseries(obs)\n.\n\n\n\n\nNote\n\n\nThe observable's memory dump contains meta information, like name, element type, element size etc., as well as time series memory chunks. The dumping is implemented in the not exported method \nMonteCarloObservable.updateondisk\n. Note that the observable's memory is \nnot\n a full backup of the observable itself (see \nsaveobs\n). Should the simulation terminate abruptly one can nonetheless restore most of the so-far recorded information using \nloadobs_frommemory\n and \ntimeseries_frommemory\n. Measurements that hadn't been dumped yet, because they were still in the preallocated buffer, are lost though.", 
            "title": "Memory / disk storage"
        }, 
        {
            "location": "/manual/memdisk/#memory-disk-storage", 
            "text": "By default the full time series of an observable is kept in memory. This is the most convenient option as it renders element access and error computation fast. However, sometimes it is preferable to track the time series on disk rather than completely in memory:   Abrupt termination: the simulation might be computationally expensive, thus slow, and might abort abruptly (maybe due to cluster outage or time limit). In this case, one probably wants to have a restorable \"memory dump\" of the so far recorded measurements to not have to restart from scratch.  Memory limit: the tracked observable might be large, i.e. a large complex matrix. Then, storing a long time series might make the simulation exceed a memory limit (and often stop unexpectedly). Keeping the time series memory on disk solves this problem.", 
            "title": "Memory / disk storage"
        }, 
        {
            "location": "/manual/memdisk/#disk-observables", 
            "text": "A \"disk observable\" is an  Observable  that every once in a while dumps it's time series memory to disk and only keeps the latest data points in memory. You can create a \"disk observable\" as  obs   =   Observable ( Float64 ,   myobservable ;   inmemory = false ,   alloc = 100 )   It will record measurements in memory until the preallocated time series buffer ( alloc=100 ) overflows in which case it saves it's time series memory to disk ( outfile ). In the above example this will happen after 100 measurements.  Apart from the special construction ( inmemory=false ) everything else stays the same as for default in-memory observables. For example, we can still get the mean via  mean(obs) , access time series elements with  obs[idx]  and load the full time series to memory at any point through  timeseries(obs) .   Note  The observable's memory dump contains meta information, like name, element type, element size etc., as well as time series memory chunks. The dumping is implemented in the not exported method  MonteCarloObservable.updateondisk . Note that the observable's memory is  not  a full backup of the observable itself (see  saveobs ). Should the simulation terminate abruptly one can nonetheless restore most of the so-far recorded information using  loadobs_frommemory  and  timeseries_frommemory . Measurements that hadn't been dumped yet, because they were still in the preallocated buffer, are lost though.", 
            "title": "\"Disk observables\""
        }, 
        {
            "location": "/methods/general/", 
            "text": "Methods: General\n\n\nBelow you find all general exports.\n\n\n\n\nIndex\n\n\n\n\nDocumentation", 
            "title": "General"
        }, 
        {
            "location": "/methods/general/#methods-general", 
            "text": "Below you find all general exports.", 
            "title": "Methods: General"
        }, 
        {
            "location": "/methods/general/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/methods/general/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/statistics/", 
            "text": "Methods: Statistics\n\n\nBelow you find all statistics related exports.\n\n\n\n\nIndex\n\n\n\n\nMonteCarloObservable.binning_error\n\n\nMonteCarloObservable.binning_error\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nMonteCarloObservable.binning_error\n \n \nMethod\n.\n\n\nbinning_error(X)\n\n\n\n\n\nEstimate of the one-sigma error of the time series's mean. Respects correlations between measurements through binning analysis.\n\n\nNote that this is not the same as \nBase.std(X)\n, not even for uncorrelated measurements.\n\n\nFor more details, see \nMonteCarloObservable.Rplateaufinder\n.\n\n\n#\n\n\nMonteCarloObservable.binning_error\n \n \nMethod\n.\n\n\nbinning_error(X, binsize)\n\n\n\n\n\nEstimate of the one-sigma error of the time series's mean. Respect correlations between measurements through binning analysis,  using the given \nbinsize\n (i.e. assuming independence of bins, Eq. 3.18 basically).", 
            "title": "Statistics"
        }, 
        {
            "location": "/methods/statistics/#methods-statistics", 
            "text": "Below you find all statistics related exports.", 
            "title": "Methods: Statistics"
        }, 
        {
            "location": "/methods/statistics/#index", 
            "text": "MonteCarloObservable.binning_error  MonteCarloObservable.binning_error", 
            "title": "Index"
        }, 
        {
            "location": "/methods/statistics/#documentation", 
            "text": "#  MonteCarloObservable.binning_error     Method .  binning_error(X)  Estimate of the one-sigma error of the time series's mean. Respects correlations between measurements through binning analysis.  Note that this is not the same as  Base.std(X) , not even for uncorrelated measurements.  For more details, see  MonteCarloObservable.Rplateaufinder .  #  MonteCarloObservable.binning_error     Method .  binning_error(X, binsize)  Estimate of the one-sigma error of the time series's mean. Respect correlations between measurements through binning analysis,  using the given  binsize  (i.e. assuming independence of bins, Eq. 3.18 basically).", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/io/", 
            "text": "Methods: IO\n\n\nBelow you find all IO related exports.\n\n\n\n\nIndex\n\n\n\n\nDocumentation", 
            "title": "IO"
        }, 
        {
            "location": "/methods/io/#methods-io", 
            "text": "Below you find all IO related exports.", 
            "title": "Methods: IO"
        }, 
        {
            "location": "/methods/io/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/methods/io/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/methods/plotting/", 
            "text": "Methods: Statistics\n\n\nBelow you find all plotting related exports.\n\n\n\n\nIndex\n\n\n\n\nDocumentation", 
            "title": "Plotting"
        }, 
        {
            "location": "/methods/plotting/#methods-statistics", 
            "text": "Below you find all plotting related exports.", 
            "title": "Methods: Statistics"
        }, 
        {
            "location": "/methods/plotting/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/methods/plotting/#documentation", 
            "text": "", 
            "title": "Documentation"
        }
    ]
}